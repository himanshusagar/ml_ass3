D:\ML\Projects\MLPython2VirtualEnv\Scripts\python.exe D:/ML/Projects/PycharmProjects/Homework3/ml_ass3/impl/impl_networks.py
MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',
       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,
       hidden_layer_sizes=(50, 100), learning_rate='constant',
       learning_rate_init=0.001, max_iter=100, momentum=0.9,
       nesterovs_momentum=True, power_t=0.5, random_state=42, shuffle=True,
       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=1,
       warm_start=False)
Iteration 1, loss = 1.75516579
Iteration 2, loss = 0.70542747
Iteration 3, loss = 0.41810211
Iteration 4, loss = 0.32233170
Iteration 5, loss = 0.27423878
Iteration 6, loss = 0.24284527
Iteration 7, loss = 0.21839232
Iteration 8, loss = 0.19883357
Iteration 9, loss = 0.18252442
Iteration 10, loss = 0.16813818
Iteration 11, loss = 0.15569200
Iteration 12, loss = 0.14433064
Iteration 13, loss = 0.13443140
Iteration 14, loss = 0.12553295
Iteration 15, loss = 0.11664929
Iteration 16, loss = 0.10984222
Iteration 17, loss = 0.10306121
Iteration 18, loss = 0.09731115
Iteration 19, loss = 0.09107658
Iteration 20, loss = 0.08570955
Iteration 21, loss = 0.08147755
Iteration 22, loss = 0.07652051
Iteration 23, loss = 0.07267832
Iteration 24, loss = 0.06833517
Iteration 25, loss = 0.06449243
Iteration 26, loss = 0.06100577
Iteration 27, loss = 0.05770185
Iteration 28, loss = 0.05447356
Iteration 29, loss = 0.05180375
Iteration 30, loss = 0.04883014
Iteration 31, loss = 0.04603200
Iteration 32, loss = 0.04382274
Iteration 33, loss = 0.04121547
Iteration 34, loss = 0.03844983
Iteration 35, loss = 0.03666691
Iteration 36, loss = 0.03438097
Iteration 37, loss = 0.03205383
Iteration 38, loss = 0.03066413
Iteration 39, loss = 0.02858050
Iteration 40, loss = 0.02677816
Iteration 41, loss = 0.02502374
Iteration 42, loss = 0.02362964
Iteration 43, loss = 0.02228625
Iteration 44, loss = 0.02084826
Iteration 45, loss = 0.01933024
Iteration 46, loss = 0.01829805
Iteration 47, loss = 0.01704652
Iteration 48, loss = 0.01580199
Iteration 49, loss = 0.01478340
Iteration 50, loss = 0.01397495
Iteration 51, loss = 0.01293353
Iteration 52, loss = 0.01213206
Iteration 53, loss = 0.01123653
Iteration 54, loss = 0.01061696
Iteration 55, loss = 0.00982615
Iteration 56, loss = 0.00916499
Iteration 57, loss = 0.00845698
Iteration 58, loss = 0.00793952
Iteration 59, loss = 0.00728234
Iteration 60, loss = 0.00687339
Iteration 61, loss = 0.00632530
Iteration 62, loss = 0.00596067
Iteration 63, loss = 0.00557593
Iteration 64, loss = 0.00522202
Iteration 65, loss = 0.00488537
Iteration 66, loss = 0.00457889
Iteration 67, loss = 0.00436529
Iteration 68, loss = 0.00411311
Iteration 69, loss = 0.00386189
Iteration 70, loss = 0.00362147
Iteration 71, loss = 0.00340538
Iteration 72, loss = 0.00332449
Iteration 73, loss = 0.00311407
Iteration 74, loss = 0.00303232
Iteration 75, loss = 0.00286012
Iteration 76, loss = 0.00271301
Iteration 77, loss = 0.00258736
Iteration 78, loss = 0.00251813
Iteration 79, loss = 0.00241361
Iteration 80, loss = 0.00234745
Iteration 81, loss = 0.00228283
Iteration 82, loss = 0.00219424
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
('iFold Accuracy : ', 0.9625052492651028, ' Best:', 0.9625052492651028)
MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',
       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,
       hidden_layer_sizes=(50, 100), learning_rate='constant',
       learning_rate_init=0.001, max_iter=100, momentum=0.9,
       nesterovs_momentum=True, power_t=0.5, random_state=42, shuffle=True,
       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=1,
       warm_start=False)
Iteration 1, loss = 1.75712390
Iteration 2, loss = 0.70728064
Iteration 3, loss = 0.41907304
Iteration 4, loss = 0.32398551
Iteration 5, loss = 0.27448107
Iteration 6, loss = 0.24164839
Iteration 7, loss = 0.21701809
Iteration 8, loss = 0.19704574
Iteration 9, loss = 0.18052365
Iteration 10, loss = 0.16717008
Iteration 11, loss = 0.15450584
Iteration 12, loss = 0.14362310
Iteration 13, loss = 0.13366844
Iteration 14, loss = 0.12481238
Iteration 15, loss = 0.11648319
Iteration 16, loss = 0.10969410
Iteration 17, loss = 0.10301941
Iteration 18, loss = 0.09697271
Iteration 19, loss = 0.09121657
Iteration 20, loss = 0.08572363
Iteration 21, loss = 0.08101393
Iteration 22, loss = 0.07577896
Iteration 23, loss = 0.07118572
Iteration 24, loss = 0.06791664
Iteration 25, loss = 0.06384228
Iteration 26, loss = 0.06010625
Iteration 27, loss = 0.05673446
Iteration 28, loss = 0.05333001
Iteration 29, loss = 0.05045476
Iteration 30, loss = 0.04729880
Iteration 31, loss = 0.04480921
Iteration 32, loss = 0.04153273
Iteration 33, loss = 0.03939792
Iteration 34, loss = 0.03709044
Iteration 35, loss = 0.03437468
Iteration 36, loss = 0.03277457
Iteration 37, loss = 0.03057948
Iteration 38, loss = 0.02854369
Iteration 39, loss = 0.02722305
Iteration 40, loss = 0.02510967
Iteration 41, loss = 0.02344828
Iteration 42, loss = 0.02240606
Iteration 43, loss = 0.02056130
Iteration 44, loss = 0.01943953
Iteration 45, loss = 0.01814846
Iteration 46, loss = 0.01696208
Iteration 47, loss = 0.01550322
Iteration 48, loss = 0.01462409
Iteration 49, loss = 0.01364537
Iteration 50, loss = 0.01301630
Iteration 51, loss = 0.01184730
Iteration 52, loss = 0.01106222
Iteration 53, loss = 0.01044865
Iteration 54, loss = 0.00986344
Iteration 55, loss = 0.00903402
Iteration 56, loss = 0.00849313
Iteration 57, loss = 0.00805811
Iteration 58, loss = 0.00741178
Iteration 59, loss = 0.00690578
Iteration 60, loss = 0.00660645
Iteration 61, loss = 0.00616192
Iteration 62, loss = 0.00579889
Iteration 63, loss = 0.00537449
Iteration 64, loss = 0.00503090
Iteration 65, loss = 0.00485190
Iteration 66, loss = 0.00434610
Iteration 67, loss = 0.00409168
Iteration 68, loss = 0.00387900
Iteration 69, loss = 0.00367314
Iteration 70, loss = 0.00348400
Iteration 71, loss = 0.00331301
Iteration 72, loss = 0.00320104
Iteration 73, loss = 0.00296116
Iteration 74, loss = 0.00286515
Iteration 75, loss = 0.00279696
Iteration 76, loss = 0.00259896
Iteration 77, loss = 0.00258101
Iteration 78, loss = 0.00244517
Iteration 79, loss = 0.00235745
Iteration 80, loss = 0.00226133
Iteration 81, loss = 0.00217740
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
('iFold Accuracy : ', 0.9594408111837763, ' Best:', 0.9625052492651028)
MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',
       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,
       hidden_layer_sizes=(50, 100), learning_rate='constant',
       learning_rate_init=0.001, max_iter=100, momentum=0.9,
       nesterovs_momentum=True, power_t=0.5, random_state=42, shuffle=True,
       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=1,
       warm_start=False)
Iteration 1, loss = 1.76075073
Iteration 2, loss = 0.70604081
Iteration 3, loss = 0.41297099
Iteration 4, loss = 0.31867064
Iteration 5, loss = 0.27105376
Iteration 6, loss = 0.23913359
Iteration 7, loss = 0.21559855
Iteration 8, loss = 0.19561613
Iteration 9, loss = 0.17876521
Iteration 10, loss = 0.16416386
Iteration 11, loss = 0.15111394
Iteration 12, loss = 0.14012279
Iteration 13, loss = 0.12949725
Iteration 14, loss = 0.11994416
Iteration 15, loss = 0.11207151
Iteration 16, loss = 0.10414643
Iteration 17, loss = 0.09766967
Iteration 18, loss = 0.09131781
Iteration 19, loss = 0.08549123
Iteration 20, loss = 0.07989310
Iteration 21, loss = 0.07534260
Iteration 22, loss = 0.07042461
Iteration 23, loss = 0.06581924
Iteration 24, loss = 0.06173354
Iteration 25, loss = 0.05870031
Iteration 26, loss = 0.05431599
Iteration 27, loss = 0.05134980
Iteration 28, loss = 0.04810909
Iteration 29, loss = 0.04520147
Iteration 30, loss = 0.04208850
Iteration 31, loss = 0.03935955
Iteration 32, loss = 0.03718345
Iteration 33, loss = 0.03442599
Iteration 34, loss = 0.03251254
Iteration 35, loss = 0.03032586
Iteration 36, loss = 0.02804808
Iteration 37, loss = 0.02634930
Iteration 38, loss = 0.02470039
Iteration 39, loss = 0.02297591
Iteration 40, loss = 0.02139118
Iteration 41, loss = 0.01976025
Iteration 42, loss = 0.01840185
Iteration 43, loss = 0.01706457
Iteration 44, loss = 0.01596845
Iteration 45, loss = 0.01455700
Iteration 46, loss = 0.01348938
Iteration 47, loss = 0.01272188
Iteration 48, loss = 0.01179397
Iteration 49, loss = 0.01097557
Iteration 50, loss = 0.01000751
Iteration 51, loss = 0.00959147
Iteration 52, loss = 0.00864465
Iteration 53, loss = 0.00821925
Iteration 54, loss = 0.00743254
Iteration 55, loss = 0.00701681
Iteration 56, loss = 0.00652374
Iteration 57, loss = 0.00634134
Iteration 58, loss = 0.00576784
Iteration 59, loss = 0.00535491
Iteration 60, loss = 0.00506658
Iteration 61, loss = 0.00471153
Iteration 62, loss = 0.00435692
Iteration 63, loss = 0.00417911
Iteration 64, loss = 0.00393660
Iteration 65, loss = 0.00373315
Iteration 66, loss = 0.00348443
Iteration 67, loss = 0.00333807
Iteration 68, loss = 0.00314566
Iteration 69, loss = 0.00292563
Iteration 70, loss = 0.00286366
Iteration 71, loss = 0.00278953
Iteration 72, loss = 0.00258643
Iteration 73, loss = 0.00246806
Iteration 74, loss = 0.00239585
Iteration 75, loss = 0.00229352
Iteration 76, loss = 0.00222251
Iteration 77, loss = 0.00216823
Iteration 78, loss = 0.00211302
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
('iFold Accuracy : ', 0.9603336533845416, ' Best:', 0.9625052492651028)
...................
('Validation Accuracy : ', 0.9658)
Iteration 83, loss = 0.19722707
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.

Process finished with exit code 0
